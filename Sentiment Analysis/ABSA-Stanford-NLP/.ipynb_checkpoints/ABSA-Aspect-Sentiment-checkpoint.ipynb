{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c75f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (3.19.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (1.20.1)\n",
      "Requirement already satisfied: six in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (1.12.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (1.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (2.25.1)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from stanza) (4.59.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msuryana\\anaconda3\\lib\\site-packages (from requests->stanza) (2020.12.5)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212371 sha256=8b5724a9484b67a55cdfd6f3a5ccace758a89a2d94d3b12157f5ad5418bd4459\n",
      "  Stored in directory: c:\\users\\msuryana\\appdata\\local\\pip\\cache\\wheels\\ae\\80\\43\\3b56e58669d65ea9ebf38b9574074ca248143b61f45e114a6b\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.1.0 stanza-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c96f367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "#import stanfordnlp\n",
    "import stanza\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ded0475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac3693ab5f6448cbc6ff13fa4e390bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac7f4a6da804d57ab8b770a6f05ff23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Finished downloading models and saved to C:\\Users\\msuryana\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cfa9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: C:\\Users\\msuryana\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\msuryana\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 235M/235M [01:57<00:00, 2.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\msuryana\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msuryana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msuryana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\msuryana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b883bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecbfaef145043ee831dd5075ce31161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "INFO:stanza:Use device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: sentiment\n",
      "INFO:stanza:Loading: constituency\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#nlp = stanfordnlp.Pipeline()\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95b2d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have downloaded the StanfordNLP English model and other essential tools using,\n",
    "# stanfordnlp.download('en')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "    \n",
    "    txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        newtaggedList = []\n",
    "        txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        wordsList = [w for w in wordsList if not w in string.punctuation]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "        #print(finaltxt)\n",
    "\n",
    "        #doc = nlp(txt) # Object of Stanford NLP Pipeline\n",
    "        doc = nlp(finaltxt) # Object of Stanford NLP Pipeline\n",
    "        #print(doc)\n",
    "        \n",
    "        # Getting the dependency relations betwwen the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            #print(dep_edge[0].text)\n",
    "            #print(dep_edge[1])\n",
    "            #print(\"------------------------------\")\n",
    "            if(not dep_edge[2].text in string.punctuation):\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "        #print(len(newwordList))\n",
    "        #print(len(dep_node))\n",
    "        # Coverting it into appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                #print(int(dep_node[i][1])\n",
    "                try:\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "                except:\n",
    "                    dep_node[i][1] = \"\"\n",
    "\n",
    "        featureList = []\n",
    "        categories = []\n",
    "        #print(taggedList)\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i)) # For features for each sentence\n",
    "                totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\n",
    "                categories.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                #print(j)\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "            \n",
    "    #print(fcluster)\n",
    "    for i in totalfeatureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return(finalcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589e4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07b9546e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>631e9f7fedf65856ab0dfe3b</td>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631e9f7fedf65856ab0dfe3e</td>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>2009-10-14 19:57:14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>631e9f7fedf65856ab0dfe42</td>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id               review_id                 user_id  \\\n",
       "0  631e9f7fedf65856ab0dfe3b  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A   \n",
       "1  631e9f7fedf65856ab0dfe3e  8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ   \n",
       "2  631e9f7fedf65856ab0dfe42  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "\n",
       "              business_id  stars  useful  funny  cool  \\\n",
       "0  04UD14gamNjLY0IDYVhHJg    1.0     1.0    2.0   1.0   \n",
       "1  RZtGWDLCAtuipwaZ-UfjmQ    4.0     0.0    0.0   0.0   \n",
       "2  kxX2SOes4o-D3ZQBkiMRfA    5.0     1.0    0.0   1.0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  I am a long term frequent customer of this est...  2015-09-23 23:10:31   \n",
       "1  Good food--loved the gnocchi with marinara\\nth...  2009-10-14 19:57:14   \n",
       "2  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n",
       "\n",
       "   compliment_count  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import reviews data\n",
    "yelp_reviews = pd.read_feather('../../FilteredData/review_philly.feather')\n",
    "yelp_reviews_filtered = yelp_reviews[['review_id', 'user_id', 'business_id', 'text', 'stars']]\n",
    "yelp_reviews_filtered['absa_pred']=yelp_reviews_filtered['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b3bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-123-30de67825545>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yelp_reviews_filtered['absa_pred'][i] = aspect_sentiment_analysis(txt, stop_words, nlp)\n",
      "<ipython-input-123-30de67825545>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yelp_reviews_filtered['absa_pred'][i] = \"FAILED\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(yelp_reviews_filtered.shape[0]):\n",
    "    txt = yelp_reviews_filtered['text'][i]\n",
    "    try:\n",
    "        yelp_reviews_filtered['absa_pred'][i] = aspect_sentiment_analysis(txt, stop_words, nlp)\n",
    "    except:\n",
    "        yelp_reviews_filtered['absa_pred'][i] = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "710de260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>absa_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[term, [long, customer]], [customer, [i, term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[good, [food, appetizer, very, food, just, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[wow, []], [yummy, []], [lambcurry, [favorite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J-4NdnDZ0pUQaUEEwDI9KQ</td>\n",
       "      <td>vrKkXsozqqecF3CW4cGaVQ</td>\n",
       "      <td>rjuWz_AD3WfXJc03AhIO_w</td>\n",
       "      <td>I thoroughly enjoyed the show.  Chill way to s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[show, [enjoyed]], [way, [chill]], [fridaynig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JBWZmBy69VMggxj3eYn17Q</td>\n",
       "      <td>aFa96pz67TwOFu4Weq5Agg</td>\n",
       "      <td>kq5Ghhh14r-eCxlVmlyd8w</td>\n",
       "      <td>My boyfriend and I tried this deli for the fir...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[boyfriend, [tried]], [timetoday, [first]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Xs8Z8lmKkosqW5mw_sVAoA</td>\n",
       "      <td>IQsF3Rc6IgCzjVV9DE8KXg</td>\n",
       "      <td>eFvzHawVJofxSnD7TgbZtg</td>\n",
       "      <td>My absolute favorite cafe in the city. Their b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[city, []], [latte, [black, best]], [amount, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>oyaMhzBSwfGgemSGuZCdwQ</td>\n",
       "      <td>Dd1jQj7S-BFGqRbApFzCFw</td>\n",
       "      <td>YtSqYv1Q_pOltsVPSx54SA</td>\n",
       "      <td>Tremendous service (Big shout out to Douglas) ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[service, [tremendous, complemented]], [shout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YcLXh-3UC9y6YFAI9xxzPQ</td>\n",
       "      <td>G0DHgkSsDozqUPWtlxVEMw</td>\n",
       "      <td>oBhJuukGRqPVvYBfTkhuZA</td>\n",
       "      <td>The only reason I didn't give this restaurant ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[reasoni, [only, give]], [restaurant, []], [s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cvQXRFLCyr0S7EgFb4lZqw</td>\n",
       "      <td>ZGjgfSvjQK886kiTzLwfLQ</td>\n",
       "      <td>EtKSTHV5Qx_Q7Aur9o4kQQ</td>\n",
       "      <td>On a scale of one to things that are awesome, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[place, [bomb]], [bomb, [place]], [drawn, []]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r2IBPY_E8AE5_GpsqlONyg</td>\n",
       "      <td>IKbjLnfBQtEyVzEu8CuOLg</td>\n",
       "      <td>VJEzpfLs_Jnzgqh5A_FVTg</td>\n",
       "      <td>It was my fiance's birthday and he decided he ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[fiance, []], [birthday, [it]], [restaurant, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A  04UD14gamNjLY0IDYVhHJg   \n",
       "1  8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ   \n",
       "2  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "3  J-4NdnDZ0pUQaUEEwDI9KQ  vrKkXsozqqecF3CW4cGaVQ  rjuWz_AD3WfXJc03AhIO_w   \n",
       "4  JBWZmBy69VMggxj3eYn17Q  aFa96pz67TwOFu4Weq5Agg  kq5Ghhh14r-eCxlVmlyd8w   \n",
       "5  Xs8Z8lmKkosqW5mw_sVAoA  IQsF3Rc6IgCzjVV9DE8KXg  eFvzHawVJofxSnD7TgbZtg   \n",
       "6  oyaMhzBSwfGgemSGuZCdwQ  Dd1jQj7S-BFGqRbApFzCFw  YtSqYv1Q_pOltsVPSx54SA   \n",
       "7  YcLXh-3UC9y6YFAI9xxzPQ  G0DHgkSsDozqUPWtlxVEMw  oBhJuukGRqPVvYBfTkhuZA   \n",
       "8  cvQXRFLCyr0S7EgFb4lZqw  ZGjgfSvjQK886kiTzLwfLQ  EtKSTHV5Qx_Q7Aur9o4kQQ   \n",
       "9  r2IBPY_E8AE5_GpsqlONyg  IKbjLnfBQtEyVzEu8CuOLg  VJEzpfLs_Jnzgqh5A_FVTg   \n",
       "\n",
       "                                                text  stars  \\\n",
       "0  I am a long term frequent customer of this est...    1.0   \n",
       "1  Good food--loved the gnocchi with marinara\\nth...    4.0   \n",
       "2  Wow!  Yummy, different,  delicious.   Our favo...    5.0   \n",
       "3  I thoroughly enjoyed the show.  Chill way to s...    5.0   \n",
       "4  My boyfriend and I tried this deli for the fir...    5.0   \n",
       "5  My absolute favorite cafe in the city. Their b...    5.0   \n",
       "6  Tremendous service (Big shout out to Douglas) ...    5.0   \n",
       "7  The only reason I didn't give this restaurant ...    4.0   \n",
       "8  On a scale of one to things that are awesome, ...    5.0   \n",
       "9  It was my fiance's birthday and he decided he ...    4.0   \n",
       "\n",
       "                                           absa_pred  \n",
       "0  [[term, [long, customer]], [customer, [i, term...  \n",
       "1  [[good, [food, appetizer, very, food, just, th...  \n",
       "2  [[wow, []], [yummy, []], [lambcurry, [favorite...  \n",
       "3  [[show, [enjoyed]], [way, [chill]], [fridaynig...  \n",
       "4  [[boyfriend, [tried]], [timetoday, [first]], [...  \n",
       "5  [[city, []], [latte, [black, best]], [amount, ...  \n",
       "6  [[service, [tremendous, complemented]], [shout...  \n",
       "7  [[reasoni, [only, give]], [restaurant, []], [s...  \n",
       "8  [[place, [bomb]], [bomb, [place]], [drawn, []]...  \n",
       "9  [[fiance, []], [birthday, [it]], [restaurant, ...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews_filtered.to_csv('predicted_absa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
